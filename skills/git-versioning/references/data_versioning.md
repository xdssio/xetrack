# Data Versioning Concepts

Core versioning concepts for the Git + DVC + xetrack system. For step-by-step workflows, see `workflows.md`. For merge/rebase details, see `merge-rebase.md`. For DVC configuration, see `dvc-setup.md`.

## Overview

Four tools form a cohesive versioning workflow:

- **Git** for code versioning, tagging, and branching
- **DVC** for large file tracking (data, models, databases), pipelines, and caching
- **xetrack** for experiment tracking via SQLite (sequential) or DuckDB (parallel) databases
- **DuckDB** as the universal merge/rebase engine for both parquet data and SQLite databases

The key insight: every experiment is a reproducible snapshot defined by a version tag. The tag points to a git commit (code), a DVC lock (data + model hashes), and a SQLite database row (results + metadata). From any tag, you can reconstruct the entire experiment.

---

## The Hash Chain

Each experiment row in the SQLite database stores:

- `version`: the semantic version string (e.g., `e0.1.0`)
- `git_commit`: the git commit hash at experiment time
- `dvc_hash`: the DVC lock file hash (pins exact data + model versions)
- `run_id` / `track_id`: xetrack's unique run identifier
- `timestamp`: when the experiment ran

This creates a bidirectional link: from any result row, trace back to exact code + data. From any tag, query exact results.

---

## Database Tables

The SQLite database uses two xetrack tables following the benchmark skill's two-table pattern:

- **`predictions`**: One row per data point per experiment. Stores individual results, raw responses, and per-item metrics. Include `version`, `git_commit`, and `dvc_hash` as tracker params so every row is traceable.
- **`metrics`**: One row per experiment run. Stores aggregated metrics (accuracy, F1, latency), experiment-level metadata, and summary statistics.

Both tables automatically include `track_id` and `timestamp` columns from xetrack. The `version` and `git_commit` fields in each row create the bidirectional link between results and code/data snapshots.

---

## Repository Structure

```
project/
├── dvc.yaml                  # DVC pipeline stages (including merge/rebase)
├── dvc.lock                  # Pinned hashes of all pipeline outputs
├── params.yaml               # Experiment parameters + merge strategy config
├── .dvc/
│   └── config                # DVC remote and cache configuration
├── data/
│   ├── train.parquet         # Training data (DVC tracked)
│   ├── test.parquet          # Test data (DVC tracked)
│   └── .gitignore            # Auto-generated by dvc add
├── models/
│   ├── production/           # Current best model
│   │   └── model.bin
│   └── candidates/           # Non-production candidate models
│       └── candidates.dvc    # Single DVC file tracking all candidates
├── results/
│   └── experiments.db        # xetrack SQLite database (DVC tracked)
├── scripts/
│   ├── merge_artifacts.py    # DuckDB-based merge/rebase for data + SQLite
│   ├── model_manager.py      # Upload/download models via SQLite metadata
│   └── version_tag.py        # Create version tags with descriptions
└── .gitignore
```

### Key Design Decisions

- `models/candidates/candidates.dvc` is a single DVC file tracking all candidate models. This keeps the main branch clean. Only the production model lives at `models/production/`.
- Scripts exist to "upload" (DVC push + tag) and "download" (DVC pull from tag) any historical model using the SQLite database as the index.
- The main branch code only has relevant files. Everything else is retrievable via SQLite + git tags.

---

## Agent Decision Flowchart

When helping a user decide which workflow and artifact strategy to use:

```
Is this a single experiment or multiple?
├── Single -> Sequential workflow
└── Multiple experiments in parallel
    ├── Same code + data, different params? -> Parallel (DuckDB, same branch)
    └── Different code or data? -> Worktree workflow

After experiments complete, for each artifact type:
├── Want to keep all results? -> Merge
├── Old results invalid (eval changed)? -> Rebase
├── Multiple model candidates? -> Merge (to candidates/)
├── Clear winner model? -> Rebase (promote to production/)
├── Adding new data samples? -> Merge
└── Schema/preprocessing changed? -> Rebase
```

---

## Important Notes

1. **Always `dvc push` before removing worktrees.** Without this, artifacts are lost.
2. **Shared DVC cache is mandatory for worktrees.** Otherwise you duplicate all data.
3. **Use `persist: true` in DVC pipeline outputs for databases.** DVC normally deletes outputs before re-running a stage.
4. **DuckDB + multiprocessing = database locks.** Use `engine='duckdb'` in xetrack for thread-safe parallel writes, but be aware DuckDB has single-writer semantics for file databases. For true multi-process parallelism, use separate database files and merge afterward.
5. **Tag descriptions are your experiment registry.** Use them liberally. `git tag -l 'e*' -n9` gives you a full experiment log.
6. **The SQLite DB is your source of truth for results.** Git tags are the index. DVC is the storage layer. Together they form a complete system.
